---
title: "Another PCA tutorial with Fisher's Iris Data"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Another PCA tutorial with Fisher's Iris Data}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, eval = T}
library(compbio4all)
```





## Important functions

* 2 functions in R for doing PCA
    + princomp()
    + prcomp()
    + difference = ?
    + princomp seems more flexible
    
## Useful packages

* FactoMineR: has PCA plotting tools
* ade4: has PCA plotting tools
    

# Data: Fisher's Irises

From R's help file:
>"This famous (Fisher's or Anderson's) iris data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica."

* 3 species
* 150 observations
* 4 characteristics
    + "Sepal.Length" 
    + "Sepal.Width"  
    + "Petal.Length"
    + "Petal.Width"  
* Characteristics highly correlated
    + plants/species with long sepals  also have long petals
    + cf (allometric scaling)[https://en.wikipedia.org/wiki/Allometry]


```{r}
data(iris)
dim(iris)
names(iris)
```

# Data visualization

## 2D visualization

* plot each of the 4 traits against each other
* "-5" drops the last columns, which is species names

```{r}
pairs(iris[,-5],lower.panel = NULL)

```


* Most traits highly correlated
    + ie, Tell me the width of a petal, and I can guess the length pretty well
* Some traits not as well correlated
    + tell me sepal width, and I won't be able to guess sepal lenght very well
    

# 3D visualziation

* We can visualization up to 3 dimensions at one time

```{r}
library(scatterplot3d)

scatterplot3d::scatterplot3d(iris$Sepal.Length,   # x axis
                 iris$Sepal.Width,     # y axis
                 iris$Petal.Length,    # z axis
               )

```


# Beyond 3D dimensions with ordination

* ordination is often called a "dimensions reduction" technique
* Take "high dimensional data" and plot in 2D or 3D while preserving key features of the original data 
* This is done by representing multidimenaionsl distances as fewer dimensions
 
Two steps

* First, run PCA
* 2nd, visualize w/"biplot"

## Run PCA

### PCA with prcomp

* note 1-sided formula
    + "~ Sepal.Length + ..."

```{r}
prcomp.iris <- prcomp(~ Sepal.Length + 
         Sepal.Width + 
         Petal.Length + 
         Petal.Width, 
         data = iris)
```


### PCA with princomp

* cor = TRUE: use correlation matrix instead of covariance matrix; relates to underlying math
* scores = TRUE: keep underlying output 
  
  
```{r}
princomp.iris <- princomp(~ Sepal.Length + 
                        Sepal.Width + 
                        Petal.Length + 
                        Petal.Width, 
                      data = iris,
                      cor = TRUE,    
                      scores = TRUE)
```
 

## Visualize with biplot

* A biplot is a visualization of the PCA output
* Data points plotted as numbers (row number)
* X axis = "principal component 1" (PC1)
* Y axis = "principal component 2" (PC2)
* These can be thought of as "latent" variables that are implied by the data (?)
* Red arrows:
    + Relationship between original traits and the new PC 
    + Arrows that overlap are highly correlated


### prcomp output
```{r}
biplot(prcomp.iris)
```


### princomp output

Difference is minor; appears to mostly be due to scaling, not a different answer.
```{r}
biplot(princomp.iris)
```


## Plotting by hand

### Extract underlying data ("scores")

```{r}
scores.PC1 <- princomp.iris$scores[,"Comp.1"]
scores.PC2 <- princomp.iris$scores[,"Comp.2"]
```


Plotting by hand; note that axes aren't standardized/scaled the same way as biplot().  To fix this just need to appropriate transformation
```{r}
plot(scores.PC1,
     scores.PC2)

```


Add scores to raw data
```{r}
iris2 <- iris
iris2$scores.PC1 <- scores.PC1
iris2$scores.PC2 <- scores.PC2


```


Plot in ggplot with species color coded
```{r}
library(ggplot2)
library(cowplot)
qplot(x = scores.PC1,
      y = scores.PC2,
      color = Species,
      data = iris2)
```



THe biplot indicates that Sepal.Width falls mostly along the y axis 
```{r}
biplot(princomp.iris)
```


This implies that it is being represent by PC2 and highly correlated with it.  We can plot the raw data for sepal with against PC2

```{r}
qplot(y = scores.PC2, 
      x = Sepal.Width, 
      data = iris2)
```



The labels might be hard to read, but petal width and petal length are pointing to the right along the x axis.  They are therefore correlated with PC1
```{r}
par(mfrow = c(1,2))
plot(scores.PC1~Petal.Length, data = iris2)
plot(scores.PC1~Petal.Width, data = iris2)
```



# Beyond PC1 and PC2

PCA creates as many new/synthetic/latent PCs as there are variables.  Typically the first 2 capture most of the interesting features of the data, but sometimes the additional ones are also useful.  A screeplot can be used to determine when PCs are becoming less informative.  when there is a steep declien between PCs, there is still information to be gained by looking at them.  The following plot implies the PC3 might be useful to look at, but not PC4.  There's a stepp drop from 1 to 2, and 2 to 3, but not 3 to 4.
```{r}
screeplot(princomp.iris)

#equivalent: 
plot(princomp.iris)

```



# Summary command

```{r}
#summary on prcomp
summary(prcomp.iris)
summary(prcomp.iris, loadings = TRUE) #doesn't do anything

#summary on princomp, the more versitile command
summary(princomp.iris)
summary(princomp.iris,    #see ?summary.princomp for details
        loadings = TRUE)


#summary with additional commands
summary(princomp.iris,         #see summary.princomp() for details
        loadings = TRUE, 
        cutoff = 0.1,
        digits = 1)
```



structure of PCA output

```{r}
#structure of PCA output
str(prcomp.iris)
str(princomp.iris)

```


# PCA by hand

* from pdf "Principal Component Analysis using R" by anon
* dated November 25, 2009


## Set up data
Create dataframe with just the numeric data (drop the )
```{r}
iris.num.data <- iris[,1:4]

```


## covariance vs.  correlation

### covariance matrix

covariances can take on any numeric value
```{r}

cov(iris.num.data)
```


### correlation matrix

correlations are all -1 to 1

```{r}
cor(iris.num.data)

```


Correlation = covariance of re-scaled data, where "scaling" is mean-centered and standardized by SD

covariance matrix on scaled data == cor matrix called on raw data


confirm this; rounding needed to get correct answer, probably due to printing errors

```{r}
round(cov(scale(iris.num.data)),10) == 
  round(cor(iris.num.data),10)

```


# eigen values

* PCA is a type of "eigen decomposition"
* eigen() command generates eigenvalues and eigenvectors
* use $ operator on eigen results to get what you want

```{r}
# all eigen... results
eigen(cov(iris.num.data))

#eigen valeus only
e.val <- eigen(cov(iris.num.data))$values

#eigen vectors only
e.vec <- eigen(cov(iris.num.data))$vectors


```


we could use covariances instead off correlations
and get different results

```{r}
#
eigen(cor(iris.num.data))$values

```


calling scale() within cov() should give us the same result

```{r}

eigen(cov(scale(iris.num.data)))$values

```


Chekc this

```{r}
round(eigen(cov(scale(iris.num.data)))$values,10) == 
  round(eigen(cor(iris.num.data))$values,10)

```


# Calculation of the the Principal Components

* this requires doing matrix multiplication
    + %*%
* of the data times the eigen vectors
* NB: data must be in matrix form, not dataframe

```{r eval = F}
#doesn't work
iris.num.data %*% e.vec
```


```{r}


# works
PC <- as.matrix(iris.num.data) %*% e.vec

# this spits out a value for every element in your
# data matrix
dim(iris.num.data) == dim(PC)


#what this is doing is multipling the data by each
#eigen vector successively

x <- cbind(as.matrix(iris.num.data) %*% e.vec[ ,1]
,as.matrix(iris.num.data) %*% e.vec[ ,2]
,as.matrix(iris.num.data) %*% e.vec[ ,3]
,as.matrix(iris.num.data) %*% e.vec[ ,4])

x == PC



#

#scaled data %*% eigen-things = "PC" object
#the eigen-vectorss are the "loadings" that 
#transform the raw data into principal component scores





# for matrix multiplication to work the number
# of columns of m1 must be equal to the number
# of rows of m2 in
# m1 %*% m2

ncol(iris.num.data) == nrow(e.vec)


#The number of ROWS of m1 determines the number of rows
# in the resulting product of the matrix mult
as.matrix(iris.num.data[1,]) %*% e.vec

as.matrix(iris.num.data[1:2,]) %*% e.vec


# the number of rows of m1 we use doesn't affect the calculation
row.1st <- as.matrix(iris.num.data[1,]) %*% e.vec
row.all <- as.matrix(iris.num.data[ ,]) %*% e.vec
row.1st == row.all[1, ]

#This is because matrix mult involves carrying
#out multiplication and addition functions
#on a row by row basis in m1
#(and a columns by column basis in m3)

#1st row of m1 by entire 1st column of m2
as.matrix(iris.num.data[1,]) %*% e.vec

#if we multiple the first row of our m1
#by just the 1st column of the our m2
#we get the FIRST element of our entire 
#"PC" object
ele.1 <- as.matrix(iris.num.data[1,]) %*% e.vec[ ,1]

ele.1 == PC[1,1]



# we could write a little program to do the addition and
# multiplication that the magic %*% operator does if
# we wanted

m1 <- as.matrix(iris.num.data) #turn data into a matrix
n.rows <- nrow(m1)
n.cols <- ncol(m1)
m2 <- e.vec
ncol(m1) == nrow(m2)

# make a blank matrix to hold results
m.out <- m1
m.out[ , ] <- NA
colnames(m.out) <- NULL
i <-1
j <-1
#loop over rows i and columns j
for (i in 1:n.rows)
{
  for (j in 1:n.cols)
  {
    m.out[i, j] <- sum(m1[i, ]*m2[,j])
  }
}


dim(m.out)
dim(PC)

# all.equal doesn't seem to be happy for some reason...
all.equal(m.out, PC)

# look for any mismatches by hand
comp.ms <- round(m.out,5) == round(PC,5)
any(comp.ms == "FALSE")



```


```{r}

######################################
###
### results of matrix mult for PCA ###
###
######################################

# a property of this new data matrix is
# that the VARIANCES of PC equal the eigen values
# (not the eigen vectors)
# and the covarainces should be (approximately) zero

#recall that in a var-covar matrix that
#the variances are on the diagonal
# the the covars are everything else

# the eigen values
e.val

#covariance matrix of object PC from our matrix multiplicatio
cov(PC)

#to view just the variances we can use diag()
diag(cov(PC))

#check this
round(diag(cov(PC)),10) == round(e.val,10)

#its a pain to write out round()
#the function all.equal() checks for 
#near equalness of all of the elements

all.equal(diag(cov(PC)), e.val)

# what happens if we call cor() on PC?
# variances (on the diagonal) are all 1
cor(PC)



### what do these data look like? 

# the original pairwise plots
pairs(iris.num.data)

#suppress upper panle
pairs(iris.num.data, upper.panel = NULL)


## FANCY: put histograms on the diagonal
panel.hist <- function(x, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(usr[1:2], 0, 1.5) )
  h <- hist(x, plot = FALSE)
  breaks <- h$breaks; nB <- length(breaks)
  y <- h$counts; y <- y/max(y)
  rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}
pairs(iris.num.data, 
      panel = panel.smooth,
      cex = 1.5, pch = 24, bg = "light blue",
      diag.panel = panel.hist, 
      cex.labels = 2, font.labels = 2)


## put (absolute) correlations on the upper panels,
## with size proportional to the correlations.
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  r <- abs(cor(x, y))
  txt <- format(c(r, 0.123456789), digits = digits)[1]
  txt <- paste0(prefix, txt)
  if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
  text(0.5, 0.5, txt, cex = cex.cor * r)
}
pairs(iris.num.data, lower.panel = panel.smooth, upper.panel = panel.cor)



#pairwise comparisons of the PC object
pairs(PC)
pairs(PC, upper.panel = NULL)
pairs(PC, lower.panel = panel.smooth, upper.panel = panel.cor)




### for more summary and distributional information than you could 
#   ever ask for:
library(ggplot2)
library(GGally)
ggpairs(iris, colour='Species', alpha=0.4)





### ###

dat.scale <- scale(iris.num.data)

t(iris.num.data) %*% PC

t(dat.scale) %*% PC

t(PC) %*% dat.scale

e.vec



### Proportion of Variance explained 
### by the different components ###

#proportion explained as a percent
e.val/sum(e.val)*100

#should add to 100%
sum(e.val/sum(e.val)*100)

#can see how quickly variance is eaten up by each
#successive component by calculating cumulative sum

cumsum(e.val/sum(e.val)*100)

# plot cumulative variance explained against
# number of components
plot(cumsum(e.val/sum(e.val)*100) ~
     c(1:length(e.val)),
     ylab = "cumulative variance explained against")



# So how does all this eigen-craziness compare to what
# prcomp acutally does

prcomp(iris.num.data)


#the standard deviations can be converted to the variances
#we computed above
#The "rotations" are the same as the eigenvectors
#we computed above

#the standard deviations that prcomp() gives
#can be converted to variances
#which are equal to the EIGENVALUES

#extract sdeviations
prcomp(iris.num.data)$sdev

#convert them to variances
prc.vars <- prcomp(iris.num.data)$sdev^2

#compar to the eigenvalues we calculated with 
#eigen() above
all.equal(e.val, prc.vars)


#so, the sd^2 from prcomp() can also give
#use the proportion of expalined variance

prc.vars/sum(prc.vars)
cumsum(prc.vars/sum(prc.vars))
```

